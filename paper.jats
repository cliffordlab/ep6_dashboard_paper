<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">0</article-id>
<article-id pub-id-type="doi">N/A</article-id>
<title-group>
<article-title>An open-source system for monitoring activity in a built
environment combining edge and fog computing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">0000-0000-0000-0000</contrib-id>
<name>
<surname>Nakum</surname>
<given-names>Arjunsinh</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>MVS</surname>
<given-names>Krishna</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Singh</surname>
<given-names>Ratan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Shu</surname>
<given-names>Nicolas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Hegde</surname>
<given-names>Chaitra</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Suresha</surname>
<given-names>Pradyumna B.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Kwon</surname>
<given-names>Dr. Hyeokhyen</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Kiarashi</surname>
<given-names>Dr. Yash</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Clifford</surname>
<given-names>Dr. Gari D.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Electrical and Computer Engineering, Georgia
Institute of Technology, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Biomedical Informatics, Emory University,
USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Biomedical Engineering, Georgia Institute of
Technology, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2017-08-13">
<day>13</day>
<month>8</month>
<year>2017</year>
</pub-date>
<volume>¿VOL?</volume>
<issue>¿ISSUE?</issue>
<fpage>¿PAGE?</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>In-door monitoring</kwd>
<kwd>Fog Computing</kwd>
<kwd>Audio Analysis</kwd>
<kwd>Vision Analysis</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Low-cost and non-contact patient monitoring system using edge
  computing platform enables longitudinal clinical studies in
  resource-limited settings. Patient monitoring systems need to capture
  both patient activities and ambient environmental conditions to
  understand patients’ conditions to provide appropriate interventions.
  In this work, we propose a low-cost indoor monitoring system that uses
  39 edge computing systems connected to a fog server layer installed in
  a built-in environment that can capture multi-modal sensors, including
  audio, video, Bluetooth strength, temperature, and humidity at
  multiple locations simultaneously. The system preserves patients’
  privacy by preprocessing the captured sensor data in real-time using
  Google Coral USB TPU to extract deidentified features before storing
  the data. The analyzed patients’ activities and ambient conditions are
  visualized in a dashboard for clinical practitioners to refer for
  patient care.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>For a distributed sensor network application like this, it becomes
  very important to ensure that all of the sensors are working
  faithfully and recording the data. To ensure this, we developed a
  robust mechanism to check the health of each of the Raspberry Pi
  (<xref alt="RaspberryPi" rid="ref-RaspberryPi" ref-type="bibr"><italic>RaspberryPi</italic></xref>)
  and sensors mounted to it. The results from this upstream system are
  sourced to the dashboard. We designed our home-built dashboard to
  answer the following needs in our study which is scalable to other
  healthcare monitoring facilities/environments : 1.Communicate
  information quickly. 2. Display information clearly and efficiently.
  3. Show trends and changes in data over time. 4. Easily customizable
  and scalable. 5. Presenting data components in a limited space.</p>
</sec>
<sec id="system-architecture">
  <title>System Architecture</title>
  <fig>
    <caption><p>Raspberry Pi attached with sensors. (A) Raspberry Pi v4,
    (B) Bluetooth sensor, (c) Temperature and humidity sensor, (d)
    Camera sensor, (3) Microphone, (e) Google Coral USB
    TPU</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/assets/Rpi_and_sensors.png" xlink:title="" />
  </fig>
  <p>The study space is installed with 39 Raspberry Pis
  (<xref alt="RaspberryPi" rid="ref-RaspberryPi" ref-type="bibr"><italic>RaspberryPi</italic></xref>)
  having multiple sensors, as shown in Figure 1. It senses Bluetooth
  beacon
  (<xref alt="Bluetooth" rid="ref-Bluetooth" ref-type="bibr"><italic>Bluetooth</italic></xref>)
  carried by patients, ambient temperature and humidity sensors
  (<xref alt="TempHumid" rid="ref-TempHumid" ref-type="bibr"><italic>TempHumid</italic></xref>),
  camera
  (<xref alt="Camera" rid="ref-Camera" ref-type="bibr"><italic>Camera</italic></xref>),
  microphone
  (<xref alt="Zuo, n.d." rid="ref-Respeaker" ref-type="bibr">Zuo,
  n.d.</xref>), and Google Coral USB TPU
  (<xref alt="Coral" rid="ref-Coral" ref-type="bibr"><italic>Coral</italic></xref>)
  for acceleration.</p>
  <fig>
    <caption><p>Overall computer network architectures.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/assets/computing_environment_hori.png" xlink:title="" />
  </fig>
  <p>Overall computer network architecture is shown in Figure 2. 39
  Raspberry Pis are transferring multi-modal sensor data to the
  on-premise fog computing node in real-time. One day volume of data
  stored in the fog computing node is nightly synced to the cloud
  computing node for permanent data storage. All sensor data is
  processed in Raspberry Pis on the device in real time. The frontend
  dashboard to monitor preprocessed sensor data are hosted in the fog
  computing node.</p>
  <fig>
    <caption><p>Architecture Diagram of Dashboard</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/assets/Architecture_Diagram.png" xlink:title="" />
  </fig>
  <p>To implement the aforementioned system, we have followed a scalable
  three-tier architecture using Flask
  (<xref alt="Grinberg, 2010" rid="ref-Flask" ref-type="bibr">Grinberg,
  2010</xref>), as shown in Figure 3, as an application server hosted
  with Nginx
  (<xref alt="Reese, 2008" rid="ref-Nginx" ref-type="bibr">Reese,
  2008</xref>) as a load balancer and reverse proxy. The frontend is
  designed with React
  (<xref alt="Uzayr, 2019" rid="ref-React" ref-type="bibr">Uzayr,
  2019</xref>) for and served through Nginx as web server. We are using
  InfluxDB
  (<xref alt="InfluxDB, 2013" rid="ref-InfluxDB" ref-type="bibr"><italic>InfluxDB</italic>,
  2013</xref>) as the database for storing the time series data
  generated by the edge devices. Redis
  (<xref alt="Redis, 2009" rid="ref-Redis" ref-type="bibr"><italic>Redis</italic>,
  2009</xref>) is used as a key-value storage to interact with
  background python
  (<xref alt="Van Rossum &amp; Drake, 2009" rid="ref-Python" ref-type="bibr">Van
  Rossum &amp; Drake, 2009</xref>) processes, whose output is consumed
  on the dashboard. MySQL
  (<xref alt="Widenius et al., 2002" rid="ref-MySQL" ref-type="bibr">Widenius
  et al., 2002</xref>) database is used for storing the authentication
  and authorization of users.</p>
</sec>
<sec id="monitoring-the-sensor-network">
  <title>Monitoring the Sensor Network</title>
  <p>The frontend <monospace>Dashboard</monospace> is a unified portal
  developed using python
  (<xref alt="Van Rossum &amp; Drake, 2009" rid="ref-Python" ref-type="bibr">Van
  Rossum &amp; Drake, 2009</xref>) packages and React
  (<xref alt="Uzayr, 2019" rid="ref-React" ref-type="bibr">Uzayr,
  2019</xref>) framework to monitor indoor activities through audio,
  visual, and spatial tracking. It monitors the following activities: 1.
  Audio 2. Visual 3. Indoor Temperature and Humidity 4. Bluetooth</p>
  <p>Section (A) in Figure 4 represents the position of each Raspberry
  Pi on our built-in environment (18,000 square feet) schematic. If
  clicked on a particular region, it shows the status of sensors
  connected to that particular Raspberry Pi as shown in section (C) of
  Figure 4. Lastly, the table in section (B) of Figure 4 represents the
  list of all Raspberry Pi with their status and an option to reboot
  them remotely.</p>
  <fig>
    <caption><p>Sensor Network Monitoring (A) Schematic of study space
    defining the positions of Raspberry Pi with region monitored by them
    (B) Status of each Raspberry Pi with option to reboot them remotely
    (C) Status of Sensors connected to Raspberry Pi in Region
    9</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/assets/EP6_Dashboard_Status_highlighted.png" xlink:title="" />
  </fig>
</sec>
<sec id="audio-pipeline-analysis">
  <title>Audio Pipeline Analysis</title>
  <fig>
    <caption><p>Audio Channels for a particular microphone
    array</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/assets/Audio_graph.png" xlink:title="" />
  </fig>
  <p>As part of the audio capture and analysis pipeline, we collect the
  environmental acoustic signals in a built-in environment through
  respeaker USB microphone arrays
  (<xref alt="Respeak?" rid="ref-Respeak" ref-type="bibr"><bold>Respeak?</bold></xref>)
  placed on the ceiling. Since the microphone can capture the
  conversations of patients, we process raw acoustic signals to extract
  unidentified acoustic features, which are Melspectogram
  (<xref alt="Lederle &amp; Wilhelm, 2018" rid="ref-Melspectogram" ref-type="bibr">Lederle
  &amp; Wilhelm, 2018</xref>) and MFCC
  (<xref alt="Lederle &amp; Wilhelm, 2018" rid="ref-Melspectogram" ref-type="bibr">Lederle
  &amp; Wilhelm, 2018</xref>), to preserve patient privacy. Through
  these features, we perform speaker diarization
  (<xref alt="Diarizaion?" rid="ref-Diarizaion" ref-type="bibr"><bold>Diarizaion?</bold></xref>)
  followed by tagging the respective participants’ groups
  (<xref alt="Tagging" rid="ref-Tagging" ref-type="bibr"><italic>Tagging</italic></xref>).
  Through these, our system can measure conversation or environmental
  audio cues related to ongoing patients’ activities. Figure 5 shows the
  power of acoustic signal measured in decibel captured at a Raspberry
  Pi.</p>
  <fig>
    <caption><p>Audio Pipeline (A) Study space schematic with the
    position of microphone arrays (B) Slider to analyze the activity
    between two specified hours (C) Heatmap depicting the activity level
    in the study space</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/assets/EP6_Dashboard_Audio.png" xlink:title="" />
  </fig>
  <p>With the detected audio activity, we conduct acoustic occupancy
  analysis, which shows overall conversation activities among the
  patients in the built-in space. Figure 6 represents the image of Audio
  section on our dashboard. Section (A) shows the physical location of
  microphone arrays in the built-in environment. To monitor hourly
  occupancy, we plot the heatmap of occupancy based on the audio signals
  captured across our study space, as shown in section (C). The slider
  in section (B) of Figure 6 can be used to change the particular time
  range for monitoring occupancy in the space.</p>
</sec>
<sec id="visual-pipeline-analysis">
  <title>Visual Pipeline Analysis</title>
  <fig>
    <caption><p>Location of individuals (red dots) within floor plan
    layout of the built environment</p></caption>
    <graphic mimetype="image" mime-subtype="jpeg" xlink:href="media/assets/EP6_occupancy.jpg" xlink:title="" />
  </fig>
  <p>The camera is used to track the movements of the patients in the
  space. Specifically, we detect 2D poses of people captured in the
  scene by using a state-of-the-art 2D pose estimation method
  (<xref alt="Papandreou et al., 2018" rid="ref-Pose2D" ref-type="bibr">Papandreou
  et al., 2018</xref>) that runs on Raspberry Pi in real-time, which can
  preserve the privacy of patients. The detected 2D poses are projected
  to the corresponding location in the study space, and the movements of
  patients are displayed in the dashboard in real-time, as shown in
  Figure 7. Monitoring patients’ movements in the space over time helps
  to understand the engagement of each patient in social interactions,
  which gives clues to patients’ mental health.</p>
  <p>Dashboard also displays the processed patient’s location data in
  heatmap. Heatmap, as the name suggests, displays the occupancy in
  terms of heat signature to visualize the population distribution
  throughout the EP6 floor. Combined with the heatmap based on the
  acoustic signal mentioned above, we can tell if the social
  interactions among patients have led to engaging conversations or not.
  The dashboard also shows the camera location associated with Raspberry
  Pi identifier number to find which camera was capturing the patient’s
  data at a specific moment.</p>
</sec>
<sec id="humidity-and-temperature-monitoring">
  <title>Humidity and Temperature Monitoring</title>
  <p>Having temperature and humidity in each partition of the health
  care facilities could provide us valuable information for various
  studies(<xref alt="Quinn &amp; Shaman, 2017" rid="ref-temp_hum" ref-type="bibr">Quinn
  &amp; Shaman, 2017</xref>). In this study, we used the DHT22
  Temperature-Humidity sensor module in conjunction with an RPi to
  record the variation in temperature and humidity. The DHT22 sensor
  comprised a thermistor and a capacitive humidity sensor that measured
  the surrounding air to provide calibrated temperature and humidity
  values. The sampling frequency is 1Hz, the temperature range was −40
  to 80 °C, and the humidity range was 0–100% RH.</p>
  <p>Figure 8 represents the temperature and humidity tab on o
  home-built dashboard. Similar to audio section, use can monitor the
  hourly occupancy, we plot the heatmap of temperature based on the
  received signal with 1Hz sampling frequency. An slider has been
  designed so the user can access the the desired time frame signal and
  heatmap. This feature can be used to monitor the relative temperature
  and humidity across the building.</p>
  <fig>
    <caption><p>Real-time monitoring of ambient temperature and humidity
    at a Raspberry Pi location.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/assets/temperature_page.png" xlink:title="" />
  </fig>
</sec>
<sec id="bluetooth-pipeline-analysis">
  <title>Bluetooth Pipeline Analysis</title>
  <p>In the Bluetooth pipeline analysis, we gather the BLE signals from
  the BLE Beacons carried by the participants through the Raspberry Pis
  placed in the ceiling. We only store the MAC address and the
  corresponding RSSI of the BLE Beacons, thus preserving participant
  privacy. Custom real-time algorithms are provided to detect the
  position of individuals wearing Bluetooth beacons moving around a
  built environment. With the collected RSSI data, we perform
  RNSI-weighted, RSSI-based Trilateration
  (<xref alt="Yang et al., 2020" rid="ref-Trilateration" ref-type="bibr">Yang
  et al., 2020</xref>) to track the movements of patients in the study
  space. Since the patients are equipped with unique BLE Beacons, we can
  associate the patient’s location tracked with BLE and camera to
  identify the exact patient that is undergoing social interactions in
  the space.</p>
  <p>Occupancy analysis of different areas in EP6 helps us correspond
  the movements of participants and their activities in Figure 8
  represents the image of Bluetooth Localisation section on EP6
  dashboard. Section (A) shows the location of Raspberry Pis in the EP6
  lab. Section B) shows the real-time location of participants in
  EP6</p>
  <p>Figure TK by Krishna and Yash</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work is part of the Cognitive Empowerment Program, which is
  supported by a generous investment from the James M. Cox Foundation
  and Cox Enterprises, Inc., in support of Emory’s Brain Health Center
  and Georgia Institute of Technology.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Trilateration">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Bo</given-names></name>
        <name><surname>Guo</surname><given-names>Luyao</given-names></name>
        <name><surname>Guo</surname><given-names>Ruijie</given-names></name>
        <name><surname>Zhao</surname><given-names>Miaomiao</given-names></name>
        <name><surname>Zhao</surname><given-names>Tiantian</given-names></name>
      </person-group>
      <article-title>A novel trilateration algorithm for RSSI-based indoor localization</article-title>
      <source>IEEE Sensors Journal</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>20</volume>
      <issue>14</issue>
      <fpage>8164</fpage>
      <lpage>8172</lpage>
    </element-citation>
  </ref>
  <ref id="ref-temp_hum">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Quinn</surname><given-names>Ashlinn</given-names></name>
        <name><surname>Shaman</surname><given-names>Jeffrey</given-names></name>
      </person-group>
      <article-title>Health symptoms in relation to temperature, humidity, and self-reported perceptions of climate in new york city residential environments</article-title>
      <source>International journal of biometeorology</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>61</volume>
      <issue>7</issue>
      <fpage>1209</fpage>
      <lpage>1220</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Pose2D">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Papandreou</surname><given-names>George</given-names></name>
        <name><surname>Zhu</surname><given-names>Tyler</given-names></name>
        <name><surname>Chen</surname><given-names>Liang-Chieh</given-names></name>
        <name><surname>Gidaris</surname><given-names>Spyros</given-names></name>
        <name><surname>Tompson</surname><given-names>Jonathan</given-names></name>
        <name><surname>Murphy</surname><given-names>Kevin</given-names></name>
      </person-group>
      <article-title>Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</article-title>
      <source>Proceedings of the european conference on computer vision (ECCV)</source>
      <year iso-8601-date="2018">2018</year>
      <fpage>269</fpage>
      <lpage>286</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Tagging">
    <element-citation publication-type="article-journal">
      <article-title>Tagging</article-title>
      <source></source>
      <uri></uri>
    </element-citation>
  </ref>
  <ref id="ref-Melspectogram">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Lederle</surname><given-names>Marcel</given-names></name>
        <name><surname>Wilhelm</surname><given-names>Benjamin</given-names></name>
      </person-group>
      <article-title>Combining high-level features of raw audio waves and mel-spectrograms for audio tagging</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <uri>https://arxiv.org/abs/1811.10708</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.1811.10708</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Respeaker">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Zuo</surname><given-names>Baozhu</given-names></name>
      </person-group>
      <article-title>Respeaker 4-MIC array for raspberry pi</article-title>
      <source>seeedstudio</source>
      <uri>https://wiki.seeedstudio.com/ReSpeaker_4_Mic_Array_for_Raspberry_Pi/</uri>
    </element-citation>
  </ref>
  <ref id="ref-RaspberryPi">
    <element-citation publication-type="book">
      <source>RaspberryPi</source>
      <uri></uri>
    </element-citation>
  </ref>
  <ref id="ref-Bluetooth">
    <element-citation publication-type="book">
      <source>Bluetooth</source>
      <uri></uri>
    </element-citation>
  </ref>
  <ref id="ref-TempHumid">
    <element-citation publication-type="book">
      <source>TempHumid</source>
      <uri></uri>
    </element-citation>
  </ref>
  <ref id="ref-Camera">
    <element-citation publication-type="book">
      <source>Camera</source>
      <uri></uri>
    </element-citation>
  </ref>
  <ref id="ref-Coral">
    <element-citation publication-type="book">
      <source>Coral</source>
      <uri></uri>
    </element-citation>
  </ref>
  <ref id="ref-Flask">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Grinberg</surname><given-names>Miguel</given-names></name>
      </person-group>
      <source>Flask: Web Development, one drop at a time</source>
      <year iso-8601-date="2010">2010</year>
      <uri>https://flask.palletsprojects.com/en/2.2.x/</uri>
    </element-citation>
  </ref>
  <ref id="ref-Python">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Van Rossum</surname><given-names>Guido</given-names></name>
        <name><surname>Drake</surname><given-names>Fred L.</given-names></name>
      </person-group>
      <source>Python 3 reference manual</source>
      <publisher-name>CreateSpace</publisher-name>
      <publisher-loc>Scotts Valley, CA</publisher-loc>
      <year iso-8601-date="2009">2009</year>
      <isbn>1441412697</isbn>
    </element-citation>
  </ref>
  <ref id="ref-React">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Uzayr</surname><given-names>Cloud bin</given-names><suffix>S.</suffix></name>
      </person-group>
      <source>React : A Javascript library for building user interfaces</source>
      <year iso-8601-date="2019">2019</year>
      <uri>https://reactjs.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-MySQL">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Widenius</surname><given-names>Michael</given-names></name>
        <name><surname>Axmark</surname><given-names>Davis</given-names></name>
        <name><surname>DuBois</surname><given-names>Paul</given-names></name>
      </person-group>
      <source>Mysql reference manual</source>
      <publisher-name>O’Reilly &amp; Associates, Inc.</publisher-name>
      <publisher-loc>USA</publisher-loc>
      <year iso-8601-date="2002">2002</year>
      <isbn>0596002653</isbn>
      <uri>https://www.mysql.com/</uri>
    </element-citation>
  </ref>
  <ref id="ref-Nginx">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Reese</surname><given-names>Will</given-names></name>
      </person-group>
      <source>Nginx: The high-performance web server and reverse proxy</source>
      <publisher-name>Belltown Media</publisher-name>
      <publisher-loc>Houston, TX</publisher-loc>
      <year iso-8601-date="2008">2008</year>
      <uri>https://www.nginx.com/</uri>
    </element-citation>
  </ref>
  <ref id="ref-InfluxDB">
    <element-citation publication-type="book">
      <source>InfluxDB</source>
      <year iso-8601-date="2013">2013</year>
      <uri>https://www.influxdata.com/</uri>
    </element-citation>
  </ref>
  <ref id="ref-Redis">
    <element-citation publication-type="book">
      <source>Redis</source>
      <year iso-8601-date="2009">2009</year>
      <uri>https://redis.io/</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
